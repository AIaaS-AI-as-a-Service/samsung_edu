{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **라이브러리 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 전처리 패키지\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 모델 패키지\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 모델 평가 패키지\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP9Qr2-36gAf"
   },
   "source": [
    "# **1. 순환 신경망(Recurrent Neural Network, RNN)**\n",
    "\n",
    "- 루프(loop)를 가진 신경망의 한 종류\n",
    "\n",
    "- 시퀀스의 원소를 순회하면서 지금까지 처리한 정보를 상태(state)에 저장\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n",
    "\n",
    "<sub>출처: https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0CNAzN_deXE"
   },
   "source": [
    "## **1.1. 실습 : IMDB 데이터 with RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMDB 데이터 셋** ###\n",
    "- 영화 리뷰 데이터\n",
    "- 영화 리뷰, 긍정/부정 레이블로 구성(이진 분류)\n",
    "- 텍스트 분석 작업에서 주로 활용\n",
    "- 50000개의 리뷰 데이터 (Train 25000, Test 25000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1SbNYRdmVTY"
   },
   "source": [
    "### **데이터 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "tokenizer = Tokenizer(num_words = 5000)\n",
    "tokenizer.fit_on_texts(train_data[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data[\"review\"]), maxlen=200)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data[\"review\"]), maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_data[\"sentiment\"]\n",
    "Y_test = test_data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128))\n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 64,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afIyKdLsmj6N"
   },
   "source": [
    "### **시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sendhEujeu8S"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b--', label = 'training loss')\n",
    "plt.plot(epochs, val_loss, 'r:', label = 'validation loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'b--', label = 'training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r:', label = 'validation accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rjMFpiBe4Fa"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTmZ_XiZguM5"
   },
   "source": [
    "**SimpleRNN의 한계점**\n",
    "\n",
    "- SimpleRNN은 실전에 사용하기엔 너무 단순\n",
    "\n",
    "- SimpleRNN은 이론적으로 시간 $t$ 에서 이전의 모든 타임스텝의 정보를 유지할 수 있지만, 실제로는 긴 시간에 걸친 의존성은 학습할 수 없음\n",
    "\n",
    "- 그래디언트 소실 문제(vanishing gradient problem)\n",
    "  - 이를 방지하기 위해 LSTM 등장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZXJIdDai7sV"
   },
   "source": [
    "# **2. LSTM(Long Short-Term Memory)**\n",
    "- 장단기 메모리 알고리즘\n",
    "\n",
    "- 나중을 위해 정보를 저장함으로써 오래된 시그널이 점차 소실되는 것을 막아줌\n",
    "\n",
    "  <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n",
    "\n",
    "  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1. 실습 : IMDB 데이터 with LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 로드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs = 5,\n",
    "                    batch_size =64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b--', label = 'training loss')\n",
    "plt.plot(epochs, val_loss, 'r:', label = 'validation loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'b--', label = 'training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r:', label = 'validation accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **실습 1. Reuters(다중 분류)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvnFLeqSsOSh"
   },
   "source": [
    "## **Reuters**\n",
    "\n",
    "- IMDB와 유사한 데이터셋(텍스트 데이터)\n",
    "- 1986년 로이터에서 공개한 짧은 뉴스 기사 및 토픽의 집합\n",
    "- 46개의 상호 배타적인 토픽으로 이루어진 데이터셋(다중 분류)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reuters with RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONPaMDFEsftq"
   },
   "source": [
    "### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "num_words = 10000\n",
    "max_len = 500\n",
    "batch_size = 32\n",
    "\n",
    "# reuters.npz 파일에서 데이터 로드\n",
    "data = np.load('reuters.npz', allow_pickle=True)\n",
    "\n",
    "# 데이터로부터 x, y변수 추출\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 차원 확인\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 전처리 및 확인**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  =Sequential()\n",
    "model.add(Embedding(input_dim = num_words, output_dim = 64))\n",
    "model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size = 32, epochs = 30,\n",
    "                    validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b--', label = 'training loss')\n",
    "plt.plot(epochs, val_loss, 'r:', label = 'validation loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'b--', label = 'training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r:', label = 'validation accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxVV-kmsT6D"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **실습2. 시계열 데이터 셋을 활용한 RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Airline-passengers dataset**\n",
    "- 1949년 1월 ~ 1960년 12월(144개월) 사이의 월간 비행기 이용객 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 로드 및 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "dataframe = pd.read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# 데이터 분리\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열의 값을 데이터셋 행렬로 변환\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=t 와 Y=t+1 로 변환\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# 배열을 [samples, time steps, features] 형태로 변환\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(SimpleRNN(units = 32, activation='tanh', input_shape=(1,look_back)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY,\n",
    "          epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# 예측 반전\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# MSE 산출\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 예측 값 plot을 위한 shift 수행\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# 테스트 예측 값 plot을 위한 shift 수행\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "day131",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
