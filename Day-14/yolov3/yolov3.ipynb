{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51bQlVgjmnWf"
   },
   "source": [
    "# **YoloV3(TensorFlow) 모델 실습**\n",
    "- TensorFlow 기반의 YoloV3 모델을 활용한 객체 탐지(Object Detection) 실습\n",
    "- VOC 데이터를 사용해 실습을 진행하며, 이미지에서 물체를 탐지하고 결과를 시각화 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 사전 환경 세팅**\n",
    "- 필요한 라이브러리 설치 및 불러오기\n",
    "- 모델 파일 및 관련 설정 파일 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIUMAdGXm46J"
   },
   "source": [
    "### **1-1. TensorFlow 정상 설치 확인**\n",
    "- TensorFlow는 딥러닝 모델을 구축 및 학습시키는 데 주로 사용되는 프레임워크\n",
    "- 설치 확인을 통해 실행 환경이 올바르게 설정되었는지 점검"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conda 환경 생성 및 활성화\n",
    "# 새로운 Conda 환경(day14-1)을 생성하고 Python 3.9 버전 설치\n",
    "\n",
    "# day14-1이라는 이름의 Conda 환경 생성\n",
    "# conda create -n day14-1 python=3.9\n",
    "\n",
    "# 생성한 day14-1 환경 활성화\n",
    "# conda activate day14-1\n",
    "\n",
    "# 필요한 패키지 설치 및 Jupyter Notebook 설정\n",
    "# requirements.txt 파일을 통해 필요한 패키지 설치\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# Jupyter Notebook 설치\n",
    "# pip install jupyter notebook\n",
    "\n",
    "# Jupyter Notebook에서 새로운 Conda 커널 설정\n",
    "# python -m ipykernel install --user --name day14-1 --display-name day14-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "oyrsdB9THu-8",
    "outputId": "a0b04911-7396-4807-af0d-04975315ac77"
   },
   "outputs": [],
   "source": [
    "# TensorFlow 라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow 버전 확인\n",
    "tf.__version__  # TensorFlow 버전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxD-M-g4nZf_"
   },
   "source": [
    "### **1-2. Darknet weight 파일로 변환 처리**\n",
    "- YOLOv3 모델의 Darknet 프레임워크에서 사용되던 가중치 파일을 TensorFlow에서 사용할 수 있도록 변환\n",
    "- Darknet은 YOLO 모델의 초기 구현에 사용된 프레임워크에 해당하며, 모델의 학습된 정보를 유지하면서 TensorFlow에서 활용할 수 있도록 가중치 파일을 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xymK200gmV25",
    "outputId": "f7c19fab-4f8b-4d5c-eed3-5b70c148065e"
   },
   "outputs": [],
   "source": [
    "# Darknet 가중치 파일을 TensorFlow 포맷으로 변환\n",
    "# YOLOv3 가중치 파일 변환\n",
    "# - 'convert.py': Darknet 가중치를 TensorFlow로 변환하는 스크립트\n",
    "# - '--weights ./data/yolov3.weights': Darknet 가중치 파일 경로를 지정\n",
    "# - '--output ./checkpoints/yolov3.tf': 변환된 TensorFlow 포맷의 출력 파일 경로\n",
    "!python convert.py --weights ./data/yolov3.weights --output ./checkpoints/yolov3.tf\n",
    "\n",
    "# YOLOv3-tiny 가중치 파일 변환\n",
    "# - YOLOv3-tiny는 경량화된 버전의 YOLOv3로, 더 빠른 처리가 가능하지만 정확도는 다소 낮을 수 있음\n",
    "# - '--tiny': 변환 대상이 YOLOv3-tiny임을 명시\n",
    "# - 다른 인자는 YOLOv3와 동일\n",
    "!python convert.py --weights ./data/yolov3-tiny.weights --output ./checkpoints/yolov3-tiny.tf --tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySGg4Ols02rX"
   },
   "source": [
    "## **2. Detector 정의**\n",
    "- 이 섹션에서는 YOLOv3 객체 탐지 모델을 정의합니다.\n",
    "- 변환된 TensorFlow 포맷의 YOLOv3 모델을 로드하고 초기화합니다.\n",
    "- Detector는 이미지를 입력받아 탐지 결과를 반환하는 주요 구성 요소입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlgBiU4ZsZY5"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 및 사용자 정의 모듈 불러오기\n",
    "\n",
    "import sys # Python 인터프리터와 상호작용하기 위한 모듈\n",
    "from absl import app, logging, flags # app: 명령줄 인터페이스를 간단히 처리, logging: 로그 메시지를 출력, flags: 명령줄 플래그를 정의하고 처리\n",
    "from absl.flags import FLAGS\n",
    "import time # 코드 실행 시간을 측정하거나 지연을 구현\n",
    "import cv2 # cv2: 컴퓨터 비전 작업(이미지 읽기, 처리 등)을 위한 모듈\n",
    "import numpy as np # np: 배열 및 수학 연산을 처리하기 위한 라이브러리\n",
    "import tensorflow as tf # tf: YOLOv3 모델 구현에 사용\n",
    "from yolov3_tf2.models import (\n",
    "    YoloV3, YoloV3Tiny\n",
    ") # YoloV3, YoloV3Tiny: YOLOv3 모델과 경량 YOLOv3-tiny 모델 정의\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset # transform_images: 이미지 전처리 함수, load_tfrecord_dataset: TFRecord 데이터셋 로드\n",
    "from yolov3_tf2.utils import draw_outputs # draw_outputs: 탐지 결과를 이미지 위에 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absl 라이브러리를 사용해 명령줄 플래그(옵션)를 정의하고, 이를 통해 YOLOv3 모델의 동작을 유연하게 제어할 수 있도록 설정 제공\n",
    "\n",
    "# 클래스 이름 파일 경로 설정\n",
    "# - COCO 데이터셋의 클래스 이름이 포함된 파일 경로를 지정\n",
    "flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\n",
    "\n",
    "# YOLOv3 모델 가중치 파일 경로 설정\n",
    "# - 변환된 YOLOv3 TensorFlow 가중치 파일 경로를 지정\n",
    "flags.DEFINE_string('weights', './checkpoints/yolov3.tf', 'path to weights file')\n",
    "\n",
    "# YOLOv3 모델 유형 선택\n",
    "# - True로 설정하면 YOLOv3-tiny 모델 사용, 기본값은 False\n",
    "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
    "\n",
    "# 입력 이미지 크기 설정\n",
    "# - YOLOv3 모델은 입력 이미지를 416x416 크기로 리사이즈하여 처리\n",
    "flags.DEFINE_integer('size', 416, 'resize images to')\n",
    "\n",
    "# 입력 이미지 파일 경로 설정\n",
    "# - 분석할 입력 이미지 파일의 경로를 지정\n",
    "flags.DEFINE_string('image', './data/girl.png', 'path to input image')\n",
    "\n",
    "# TFRecord 데이터셋 경로 설정 (선택 사항)\n",
    "# - TFRecord 형식의 데이터셋 경로를 지정, 기본값은 None\n",
    "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
    "\n",
    "# 출력 이미지 파일 경로 설정\n",
    "# - 탐지 결과가 저장될 이미지 파일 경로를 지정\n",
    "flags.DEFINE_string('output', './output.jpg', 'path to output image')\n",
    "\n",
    "# 모델 클래스 수 설정\n",
    "# - YOLOv3 모델의 클래스 개수를 지정 (기본값: 80)\n",
    "flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n",
    "\n",
    "# 앱 초기화 및 명령줄 플래그 파싱\n",
    "# - 앱 실행 준비 및 플래그 값 파싱\n",
    "app._run_init(['yolov3'], app.parse_flags_with_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정을 통해 사용 가능한 GPU 장치를 확인하고, GPU 메모리 활성화\n",
    "\n",
    "# 사용 가능한 GPU 장치 확인\n",
    "# 'tf.config.experimental.list_physical_devices('GPU')'는 시스템에서 사용 가능한 GPU 장치 목록 반환\n",
    "# 이를 통해 GPU가 올바르게 감지되었는지 확인\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# GPU 메모리 동적 할당 설정\n",
    "# 'set_memory_growth': GPU 메모리를 필요한 만큼만 동적으로 할당\n",
    "# 일반적으로 TensorFlow는 실행 시 모든 GPU 메모리를 예약하지만, 동적 설정을 통해 효율적인 메모리 사용이 가능\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laxApAGV07kw"
   },
   "source": [
    "## **3. pre-trained weight를 활용한 detection 테스트**\n",
    "- 이 섹션에서는 사전 학습된 YOLOv3 가중치를 사용하여 객체 탐지 작업을 수행합니다.\n",
    "- 사전 학습된 가중치는 COCO 데이터셋에서 훈련된 YOLOv3 모델의 학습 정보를 포함합니다.\n",
    "- 이를 통해 모델의 성능을 확인하고 탐지 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "colab_type": "code",
    "id": "2iKC1pvBnkDk",
    "outputId": "b15566f3-7c22-433e-c57b-d0e508d794cb"
   },
   "outputs": [],
   "source": [
    "# 테스트할 이미지 입력 경로 설정\n",
    "# FLAGS.image를 통해 탐지 테스트에 사용할 입력 이미지의 경로 지정\n",
    "# 'data/meme.jpg'은 샘플 이미지 경로\n",
    "FLAGS.image = 'data/street_out.jpg' # meme2, girl, street_out, street 로도 테스트 \n",
    "\n",
    "# YOLOv3 기본 또는 tiny 모델 파이프라인 선택\n",
    "# FLAGS.tiny: True이면 YOLOv3-tiny 모델을 사용, False이면 일반 YOLOv3 모델을 사용\n",
    "# classes: 모델이 탐지할 클래스 수를 설정 (기본 80 클래스)\n",
    "if FLAGS.tiny:\n",
    "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)  # 경량 YOLOv3-tiny 모델 생성\n",
    "else:\n",
    "    yolo = YoloV3(classes=FLAGS.num_classes)  # YOLOv3 모델 생성\n",
    "\n",
    "# 모델 weight 파일 로드\n",
    "# FLAGS.weights 경로에 저장된 사전 학습된 가중치를 모델에 로드\n",
    "# expect_partial(): 일부 레이어가 맞지 않을 경우에도 로드 허용\n",
    "yolo.load_weights(FLAGS.weights).expect_partial()\n",
    "logging.info('weights loaded')  # 가중치 로드 완료 메시지\n",
    "\n",
    "# 클래스 이름 로드\n",
    "# FLAGS.classes 파일에 저장된 클래스 이름을 리스트로 읽어오기\n",
    "# c.strip(): 클래스 이름에서 불필요한 공백 제거\n",
    "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
    "logging.info('classes loaded')  # 클래스 로드 완료 메시지\n",
    "\n",
    "# 입력 이미지 로드\n",
    "# FLAGS.image 경로에서 이미지를 바이너리 모드로 읽어와 디코딩\n",
    "# channels=3: 이미지를 RGB 채널로 디코딩\n",
    "img_raw = tf.image.decode_image(\n",
    "    open(FLAGS.image, 'rb').read(), channels=3)\n",
    "\n",
    "# 이미지 전처리\n",
    "# 차원 확장: 모델에 입력하기 위해 배치 차원을 추가\n",
    "img = tf.expand_dims(img_raw, 0)\n",
    "# 리사이즈 및 정규화: 이미지를 YOLOv3의 입력 크기(FLAGS.size)로 리사이즈하고 픽셀 값을 정규화\n",
    "img = transform_images(img, FLAGS.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 수행\n",
    "\n",
    "# 시작 시간 기록\n",
    "# 'time.time()'을 사용해 객체 탐지 수행 전의 시간 기록\n",
    "t1 = time.time()\n",
    "\n",
    "# 객체 탐지 수행\n",
    "# yolo(img): YOLOv3 모델이 입력 이미지를 분석하고 객체 탐지\n",
    "# 반환값:\n",
    "# - boxes: 탐지된 객체의 경계 상자 좌표\n",
    "# - scores: 각 객체의 신뢰도 점수\n",
    "# - classes: 탐지된 객체의 클래스 번호\n",
    "# - nums: 탐지된 객체의 총 개수\n",
    "boxes, scores, classes, nums = yolo(img)\n",
    "\n",
    "# 종료 시간 기록\n",
    "# 탐지 작업 완료 시점 기록\n",
    "t2 = time.time()\n",
    "\n",
    "# 탐지 시간 출력\n",
    "# 탐지 수행 시간(t2 - t1) 출력\n",
    "logging.info('time: {}'.format(t2 - t1))\n",
    "\n",
    "# 탐지된 객체 정보 출력\n",
    "# 'detections:' 메시지 출력\n",
    "# - 탐지된 객체 정보를 출력하기 전 시작 메시지 로그\n",
    "logging.info('detections:')\n",
    "\n",
    "# 탐지된 객체를 하나씩 출력\n",
    "# - nums[0]: 탐지된 객체의 총 개수\n",
    "# - 클래스 이름, 신뢰도 점수, 경계 상자 좌표를 순서대로 로그에 출력\n",
    "for i in range(nums[0]):\n",
    "    logging.info('\\t{}, {}, {}'.format(\n",
    "        class_names[int(classes[0][i])],  # 클래스 이름\n",
    "        np.array(scores[0][i]),          # 신뢰도 점수\n",
    "        np.array(boxes[0][i])            # 경계 상자 좌표\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐지 결과 이미지로 표시\n",
    "\n",
    "# 이미지 색상 변환\n",
    "# - OpenCV는 BGR 형식을 사용하므로, TensorFlow로 불러온 RGB 이미지를 BGR로 변환\n",
    "# - img_raw.numpy(): TensorFlow 텐서를 NumPy 배열로 변환\n",
    "img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# 탐지 결과 시각화\n",
    "# - draw_outputs(): 탐지된 경계 상자와 클래스 이름, 신뢰도 점수를 이미지 위에 표시\n",
    "# - img: 원본 이미지\n",
    "# - (boxes, scores, classes, nums): 탐지된 객체 정보\n",
    "# - class_names: 클래스 이름 리스트\n",
    "img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n",
    "\n",
    "# Jupyter Notebook에서 이미지 표시\n",
    "# IPython.display 모듈 불러오기\n",
    "# - Jupyter Notebook에서 이미지를 표시하기 위한 모듈\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 이미지 디스플레이\n",
    "# - cv2.imencode(): 이미지를 특정 형식(예: JPEG)으로 인코딩\n",
    "# - bytes(): 인코딩된 이미지를 바이트로 변환\n",
    "# - width=800: 표시할 이미지의 너비를 설정\n",
    "display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Up4Xcad81FSa"
   },
   "source": [
    "## **4. 신규 학습 진행**\n",
    "- 새로운 데이터셋을 사용하여 YOLOv3 모델 학습\n",
    "- 사전 학습된 가중치를 활용하거나 초기화된 모델로 시작 가능\n",
    "- 학습된 모델은 사용자 정의 데이터셋에서 물체를 탐지하도록 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-1. 데이터 전처리**\n",
    "- 학습에 사용할 데이터셋을 모델 입력 형식에 맞게 변환\n",
    "- 이미지 크기 조정, 정규화, TFRecord 로드 등 전처리 작업에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 생성할 위치(/Day-14/yolov3)로 이동\n",
    "# %cd /{}/Day-14/yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-I8Ml-j4Iyuv",
    "outputId": "a1880906-4c01-4f08-a12b-ca2f37582a55"
   },
   "outputs": [],
   "source": [
    "# raw 데이터(tar) 압축 해제\n",
    "\n",
    "# 데이터셋 다운로드 (S3 드라이브에서 voc2009_raw.tar 파일 다운로드, Day-14/yolov3/data/ 위치로 이동)\n",
    "# - wget 명령으로 Pascal VOC 2009 데이터셋을 다운로드\n",
    "# - '-O ./data/voc2009_raw.tar': 다운로드한 파일을 지정된 경로에 저장\n",
    "# !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar -O ./data/voc2009_raw.tar\n",
    "\n",
    "# 데이터셋 디렉토리 생성\n",
    "# - 'mkdir -p': 지정된 경로에 디렉토리를 생성 (이미 존재하면 무시)\n",
    "!mkdir -p ./data/voc2009_raw\n",
    "\n",
    "# tar 파일 압축 해제\n",
    "# - 'tar -xf': 지정된 tar 파일을 해제\n",
    "# - '-C ./data/voc2009_raw': 압축 해제된 데이터를 지정된 디렉토리로 저장\n",
    "!tar -xf ./data/voc2009_raw.tar -C ./data/voc2009_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "9lvttM39I5Na",
    "outputId": "faa0df6f-ab49-4476-b2e7-de3c4dbdb5d3"
   },
   "outputs": [],
   "source": [
    "# YOLOv3 학습을 위한 voc2012 학습 데이터 전처리 수행\n",
    "\n",
    "# 1. voc2012.py 스크립트를 실행하여 데이터셋을 변환\n",
    "# - '--data_dir': 원본 데이터셋 경로를 지정\n",
    "# - '--split': 사용할 데이터 분할을 지정 (train은 학습용 데이터)\n",
    "# - '--output_file': 변환된 TFRecord 파일의 저장 경로\n",
    "!python3 tools/voc2012.py \\\n",
    "  --data_dir './data/voc2009_raw/VOCdevkit/VOC2009' \\\n",
    "  --split train \\\n",
    "  --output_file ./data/voc_train.tfrecord\n",
    "\n",
    "# YOLOv3 학습을 위한 voc2012 검증 데이터 전처리 수행\n",
    "\n",
    "# 1. voc2012.py 스크립트를 실행하여 검증용 데이터를 변환\n",
    "# - '--split val': 검증용 데이터를 지정\n",
    "# - 결과는 './data/voc_val.tfrecord'에 저장\n",
    "!python3 tools/voc2012.py \\\n",
    "  --data_dir './data/voc2009_raw/VOCdevkit/VOC2009' \\\n",
    "  --split val \\\n",
    "  --output_file ./data/voc_val.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "ZBhryo1I2dwG",
    "outputId": "79963b6f-7f30-4a3d-dd83-792ed28a790d"
   },
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정 및 학습 시작\n",
    "\n",
    "# 1. 학습 스크립트 실행\n",
    "# - 'train.py' 스크립트를 실행하여 YOLOv3 모델 학습 시작\n",
    "\n",
    "# 2. 주요 파라미터\n",
    "# - '--dataset': 학습 데이터셋(TFRecord 파일) 경로\n",
    "# - '--val_dataset': 검증 데이터셋(TFRecord 파일) 경로\n",
    "# - '--classes': 클래스 이름 파일 경로\n",
    "# - '--num_classes': 학습할 데이터셋의 클래스 수 (Pascal VOC는 20개 클래스)\n",
    "# - '--mode fit': Keras 모델 학습 모드 지정\n",
    "# - '--transfer darknet': Darknet 가중치를 전이 학습으로 사용\n",
    "# - '--batch_size': 학습에 사용할 배치 크기\n",
    "# - '--epochs': 학습 반복 횟수\n",
    "# - '--weights': 초기 가중치 파일 경로\n",
    "# - '--weights_num_classes': 초기 가중치에 포함된 클래스 수 (COCO 데이터셋은 80개 클래스)\n",
    "\n",
    "!python3 train.py \\\n",
    "\t--dataset ./data/voc_train.tfrecord \\\n",
    "\t--val_dataset ./data/voc_val.tfrecord \\\n",
    "\t--classes ./data/voc2012.names \\\n",
    "\t--num_classes 20 \\\n",
    "\t--mode fit --transfer darknet \\\n",
    "\t--batch_size 8 \\\n",
    "\t--epochs 3 \\\n",
    "\t--weights ./checkpoints/yolov3.tf \\\n",
    "\t--weights_num_classes 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgtlCN1m1TKG"
   },
   "source": [
    "## **5. 신규 학습 weight를 활용한 추론**\n",
    "- 이 섹션에서는 새롭게 학습된 YOLOv3 가중치를 사용하여 객체 탐지를 수행합니다.\n",
    "- 학습된 모델의 성능을 검증하고 탐지 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "wok7x44vNYuP",
    "outputId": "45e152d6-041a-48c9-813d-9d1588d5b4b8"
   },
   "outputs": [],
   "source": [
    "# 새로 학습된 모델을 활용한 추론 준비\n",
    "\n",
    "# 모델이 인식할 클래스 수 설정\n",
    "# - Pascal VOC 데이터셋은 총 20개의 클래스로 구성\n",
    "FLAGS.num_classes = 20\n",
    "\n",
    "# 클래스 이름 파일 경로 설정\n",
    "# - 'data/voc2012.names' 파일 내 20개 클래스 이름 정의\n",
    "FLAGS.classes = 'data/voc2012.names'\n",
    "\n",
    "# 학습된 가중치 파일 경로 설정\n",
    "# - 'checkpoints/yolov3_train_3.tf' 파일은 3 에포크 동안 학습된 YOLOv3 모델의 가중치를 저장한 파일\n",
    "FLAGS.weights = 'checkpoints/yolov3_train_3.tf'\n",
    "\n",
    "# 추론에 사용할 입력 이미지 경로 설정\n",
    "# - 'data/meme.jpg'는 추론 테스트에 사용될 입력 이미지 파일\n",
    "FLAGS.image = 'data/meme.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esTFKZzX7jYj"
   },
   "outputs": [],
   "source": [
    "# 낮은 임계값 설정 및 추론 진행\n",
    "\n",
    "# IoU 임계값 설정\n",
    "# - IoU(Intersection over Union): 예측된 바운딩 박스와 실제 바운딩 박스 간의 겹치는 비율\n",
    "# - 낮은 임계값(0.1)을 설정하여 더 많은 바운딩 박스를 탐지\n",
    "FLAGS.yolo_iou_threshold = 0.1\n",
    "\n",
    "# 점수 임계값 설정\n",
    "# - 예측된 바운딩 박스의 신뢰도 점수 임계값\n",
    "# - 낮은 임계값(0.1)을 설정하여 더 많은 바운딩 박스를 표시\n",
    "FLAGS.yolo_score_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 YOLOv3 Tiny 또는 기본 YOLOv3로 설정\n",
    "\n",
    "# 1. 모델 초기화\n",
    "# - FLAGS.tiny가 True이면 YOLOv3 Tiny 모델을 사용\n",
    "# - False이면 YOLOv3 기본 모델을 사용\n",
    "if FLAGS.tiny:\n",
    "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)  # YOLOv3 Tiny 모델 초기화\n",
    "else:\n",
    "    yolo = YoloV3(classes=FLAGS.num_classes)  # YOLOv3 기본 모델 초기화\n",
    "\n",
    "# 학습된 가중치 로드\n",
    "# - FLAGS.weights 경로에서 학습된 가중치를 모델에 로드\n",
    "# - expect_partial(): 일부 레이어가 맞지 않을 경우에도 로드 허용\n",
    "yolo.load_weights(FLAGS.weights).expect_partial()\n",
    "logging.info('weights loaded')  # 가중치 로드 완료 메시지\n",
    "\n",
    "# 클래스 이름을 로드\n",
    "# - FLAGS.classes 경로에 정의된 클래스 이름을 리스트로 읽어옴\n",
    "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
    "logging.info('classes loaded')  # 클래스 로드 완료 메시지\n",
    "\n",
    "# 입력 이미지를 읽고 디코딩\n",
    "# - FLAGS.image 경로에서 이미지를 바이너리 모드로 읽어와 디코딩\n",
    "# - channels=3: 이미지를 RGB 채널로 디코딩\n",
    "img_raw = tf.image.decode_image(\n",
    "    open(FLAGS.image, 'rb').read(), channels=3\n",
    ")\n",
    "\n",
    "# 배치 차원을 추가하여 모델에 맞게 확장\n",
    "# - YOLOv3 모델은 배치 입력을 요구하므로 차원을 확장\n",
    "img = tf.expand_dims(img_raw, 0)\n",
    "\n",
    "# 이미지를 모델 입력 크기로 변환\n",
    "# - FLAGS.size에 설정된 크기 (예: 416x416)로 리사이즈 및 정규화\n",
    "img = transform_images(img, FLAGS.size)\n",
    "\n",
    "# 예측 수행\n",
    "\n",
    "# 1. 예측 시작 시간 기록\n",
    "t1 = time.time()\n",
    "\n",
    "# 2. 모델을 사용하여 입력 이미지에서 객체 탐지\n",
    "# - boxes: 탐지된 바운딩 박스 좌표\n",
    "# - scores: 각 객체의 신뢰도 점수\n",
    "# - classes: 탐지된 객체의 클래스 번호\n",
    "# - nums: 탐지된 객체의 총 개수\n",
    "boxes, scores, classes, nums = yolo(img)\n",
    "\n",
    "# 3. 예측 종료 시간 기록\n",
    "t2 = time.time()\n",
    "logging.info('time: {}'.format(t2 - t1))  # 탐지 시간 출력\n",
    "\n",
    "# 탐지 결과 로깅\n",
    "\n",
    "# 1. 탐지된 객체 정보 출력 시작\n",
    "logging.info('detections:')\n",
    "\n",
    "# 2. 탐지된 객체의 클래스, 신뢰도 점수, 바운딩 박스 좌표를 하나씩 출력\n",
    "for i in range(nums[0]):\n",
    "    logging.info('\\t{}, {}, {}'.format(\n",
    "        class_names[int(classes[0][i])],  # 클래스 이름\n",
    "        np.array(scores[0][i]),          # 신뢰도 점수\n",
    "        np.array(boxes[0][i])            # 바운딩 박스 좌표\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 색상 변환 (RGB에서 BGR로 변환하여 OpenCV에서 사용)\n",
    "# - TensorFlow에서 로드된 이미지는 RGB 형식\n",
    "# - OpenCV는 BGR 형식을 사용하므로 변환이 필요\n",
    "img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# 예측된 바운딩 박스와 클래스 이름을 이미지에 그림\n",
    "# - draw_outputs(): 탐지된 객체 정보를 이미지 위에 시각화\n",
    "# - img: 입력 이미지\n",
    "# - (boxes, scores, classes, nums): 탐지 결과\n",
    "# - class_names: 클래스 이름 리스트\n",
    "img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n",
    "\n",
    "# 결과 이미지를 파일로 저장\n",
    "# - FLAGS.output: 저장할 파일 경로\n",
    "cv2.imwrite(FLAGS.output, img)\n",
    "logging.info('output saved to: {}'.format(FLAGS.output))  # 결과 저장 로그\n",
    "\n",
    "# Jupyter Notebook에서 이미지를 표시\n",
    "# - IPython.display 모듈을 사용하여 이미지 출력\n",
    "from IPython.display import Image, display\n",
    "# - cv2.imencode(): 이미지를 JPEG 형식으로 인코딩\n",
    "# - bytes(): 인코딩된 이미지를 바이트로 변환\n",
    "# - width=800: 출력 이미지의 너비 설정\n",
    "display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "colab_gpu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "day14-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
