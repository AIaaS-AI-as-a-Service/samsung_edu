{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **실습 문제**\n",
    "* 주어진 코드는 새의 품종을 분류하는 코드\n",
    "* 랜덤 서치를 이용하여 하이퍼 파라미터 튜닝 수행\n",
    "* 하이퍼 파라미터의 개수는 최소 3개 이상 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **# 1. 데이터셋 경로 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 경로 설정\n",
    "original_data_dir = \"cifar10_images\" \n",
    "output_base_dir = \"cifar10_dataset\"\n",
    "train_dir = os.path.join(output_base_dir, \"train\")\n",
    "valid_dir = os.path.join(output_base_dir, \"valid\")\n",
    "test_dir = os.path.join(output_base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 데이터셋 분할: 0.7 (Train), 0.2 (Validation), 0.1 (Test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(original_dir, train_ratio=0.7, valid_ratio=0.2):\n",
    "    # 클래스별 데이터를 나눔\n",
    "    class_dirs = [os.path.join(original_dir, class_name) for class_name in os.listdir(original_dir)]\n",
    "    for class_dir in class_dirs:\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        # 각 클래스의 이미지 경로\n",
    "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('.png', '.jpg'))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # 분할\n",
    "        train_split = int(len(images) * train_ratio)\n",
    "        valid_split = int(len(images) * (train_ratio + valid_ratio))\n",
    "\n",
    "        train_images = images[:train_split]\n",
    "        valid_images = images[train_split:valid_split]\n",
    "        test_images = images[valid_split:]\n",
    "\n",
    "        # 데이터를 출력 디렉토리에 저장\n",
    "        for output_dir, image_set in zip([train_dir, valid_dir, test_dir], [train_images, valid_images, test_images]):\n",
    "            class_output_dir = os.path.join(output_dir, os.path.basename(class_dir))\n",
    "            os.makedirs(class_output_dir, exist_ok=True)\n",
    "            for image_path in image_set:\n",
    "                shutil.copy(image_path, class_output_dir)\n",
    "\n",
    "split_data(original_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "train_folder='Birds/train'\n",
    "test_folder='Birds/test'\n",
    "\n",
    "class_reduce=0.1 # 부류 수 줄여서 데이터양 줄임(속도와 메모리 효율을 위해)\n",
    "no_class=int(len(os.listdir(train_folder))*class_reduce) # 부류 개수\n",
    "\n",
    "x_train,y_train=[],[]\n",
    "for i,class_name in enumerate(os.listdir(train_folder)):\n",
    "    if i<no_class: # 13~14행이 지정한 부류만 사용\n",
    "        for fname in os.listdir(train_folder+'/'+class_name):\n",
    "            img=image.load_img(train_folder+'/'+class_name+'/'+fname,target_size=(224,224))\n",
    "            if len(img.getbands())!=3:\n",
    "                print(\"주의: 유효하지 않은 영상 발생\",class_name,fname)\n",
    "                continue\n",
    "            x=image.img_to_array(img)\n",
    "            x=preprocess_input(x)\n",
    "            x_train.append(x)\n",
    "            y_train.append(i)\n",
    "\n",
    "x_test,y_test=[],[]\n",
    "for i,class_name in enumerate(os.listdir(test_folder)):\n",
    "    if i<no_class: # 13~14행이 지정한 부류만 사용\n",
    "        for fname in os.listdir(test_folder+'/'+class_name):\n",
    "            img=image.load_img(test_folder+'/'+class_name+'/'+fname,target_size=(224,224))\n",
    "            if len(img.getbands())!=3:\n",
    "                print(\"주의: 유효하지 않은 영상 발생\",class_name,fname)\n",
    "                continue\n",
    "            x=image.img_to_array(img)\n",
    "            x=preprocess_input(x)\n",
    "            x_test.append(x)\n",
    "            y_test.append(i)\n",
    "\n",
    "x_train=np.asarray(x_train)\n",
    "y_train=np.asarray(y_train)\n",
    "x_test=np.asarray(x_test)\n",
    "y_test=np.asarray(y_test)\n",
    "y_train=tf.keras.utils.to_categorical(y_train,no_class)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,no_class)\n",
    "\n",
    "base_model=ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
    "cnn=Sequential()\n",
    "cnn.add(base_model)\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024,activation='relu'))\n",
    "cnn.add(Dense(no_class,activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',optimizer=Adam(0.00002),metrics=['accuracy'])\n",
    "hist=cnn.fit(x_train,y_train,batch_size=16,epochs=10,validation_data=(x_test,y_test),verbose=1)\n",
    "\n",
    "res=cnn.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"정확률은\",res[1]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "day16_1_jupyter",
   "language": "python",
   "name": "day16_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
