{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **최적화(Optimization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 02:52:58.962026: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 02:53:00.192245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 02:53:03.306008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 02:53:03.504350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 02:53:03.504442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD,Adam,Adagrad,RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. 데이터 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **# 1. 데이터셋 경로 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 경로 설정\n",
    "original_data_dir = \"fashion_mnist_images\"  \n",
    "output_base_dir = \"fashion_mnist_dataset\"\n",
    "train_dir = os.path.join(output_base_dir, \"train\")\n",
    "valid_dir = os.path.join(output_base_dir, \"valid\")\n",
    "test_dir = os.path.join(output_base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in os.listdir(original_data_dir):\n",
    "    print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 데이터셋 분할: 0.7 (Train), 0.2 (Validation), 0.1 (Test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_data_stratified(original_dir, train_ratio=0.7, valid_ratio=0.2):\n",
    "    # 모든 데이터를 모으고 라벨 생성\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(original_dir):\n",
    "        class_dir = os.path.join(original_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if img_name.endswith(('.png', '.jpg')):\n",
    "                image_paths.append(os.path.join(class_dir, img_name))\n",
    "                labels.append(class_name)\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "        image_paths, labels, test_size=(1 - train_ratio), stratify=labels, random_state=42\n",
    "    )\n",
    "    valid_size = valid_ratio / (1 - train_ratio)  # validation 비율 조정\n",
    "    valid_images, test_images, valid_labels, test_labels = train_test_split(\n",
    "        temp_images, temp_labels, test_size=(1 - valid_size), stratify=temp_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # 데이터를 출력 디렉토리에 저장\n",
    "    for dataset, output_dir in zip(\n",
    "        [(train_images, train_labels), (valid_images, valid_labels), (test_images, test_labels)],\n",
    "        [train_dir, valid_dir, test_dir]\n",
    "    ):\n",
    "        images, labels = dataset\n",
    "        for img_path, label in zip(images, labels):\n",
    "            class_output_dir = os.path.join(output_dir, label)\n",
    "            os.makedirs(class_output_dir, exist_ok=True)\n",
    "            shutil.copy(img_path, class_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_stratified(original_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 과적합(Overfitting)**\n",
    "* 모델이 학습 데이터에 대한 성능은 매우 우수하지만, 새로운 데이터에 대해서는 일반화 성능이 떨어지는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Fitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-2. 과적합 해결방안: 데이터 증강(Data Augmentation)**\n",
    "* 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 학습\n",
    "* 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합 방지\n",
    "* Train Data에 대해서만 데이터 증강 수행(Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 증강(Data Augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgaug의 iaa.Sequential을 사용하여 다양한 증강 기법을 적용\n",
    "def augment_images(input_dir, output_dir):\n",
    "    \n",
    "    # Augmentation 설정\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),  # 수평 반전\n",
    "        #iaa.Flipud(0.5),  # 수직 반전\n",
    "        iaa.Affine(rotate=(-15, 15)),  # -15도에서 15도까지 회전\n",
    "        #iaa.Multiply((0.8, 1.2)),  # 밝기 조정\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0))  # 가우시안 블러\n",
    "    ])\n",
    "\n",
    "    class_dirs = [os.path.join(input_dir, class_name) for class_name in os.listdir(input_dir)]\n",
    "    for class_dir in class_dirs:\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        output_class_dir = os.path.join(output_dir, os.path.basename(class_dir))\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        # 이미지를 읽어서 증강 후 저장\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if not img_path.endswith(('.png', '.jpg')):\n",
    "                continue\n",
    "\n",
    "            # 이미지를 불러와 증강\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "            augmented_images = seq(images=[image_np] * 1)  # 이미지 1개 생성\n",
    "\n",
    "            for i, aug_img in enumerate(augmented_images):\n",
    "                aug_img = Image.fromarray(aug_img)\n",
    "                aug_img.save(os.path.join(output_class_dir, f\"{img_name.split('.')[0]}_aug_{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train='fashion_mnist_dataset/train'\n",
    "output_train='augmented_fashion_mnist_dataset/train'\n",
    "augment_images(input_train, output_train)\n",
    "print(\"Train 데이터에 대한 Augmentation 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **증강된 이미지 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 증강된 데이터 경로 설정\n",
    "augmented_data_dir = \"augmented_fashion_mnist_dataset/train\"  # 증강 데이터가 저장된 디렉토리\n",
    "class_name = \"Ankle boot\"  # 확인할 클래스 이름\n",
    "class_dir = os.path.join(augmented_data_dir, class_name)\n",
    "\n",
    "# 특정 클래스 디렉토리에서 이미지 읽기\n",
    "images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('.png', '.jpg'))]\n",
    "\n",
    "# 확인할 이미지 선택\n",
    "num_images_to_display = 5  # 확인할 이미지 수\n",
    "selected_images = images[:num_images_to_display]\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, img_path in enumerate(selected_images):\n",
    "    # 파일 이름 추출\n",
    "    image_name = os.path.basename(img_path)\n",
    "    \n",
    "    # PIL로 이미지 열기\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.subplot(1, num_images_to_display, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(image_name)  # 이미지 파일 이름을 제목으로 표시\n",
    "    \n",
    "    # 파일 이름 출력\n",
    "    print(f\"Displaying: {image_name}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **증강 전후 결과 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "original_train_dir = \"fashion_mnist_dataset/train\"\n",
    "augmented_train_dir = \"augmented_fashion_mnist_dataset/train\"\n",
    "valid_dir = \"fashion_mnist_dataset/valid\"  # 증강 후에도 동일한 validation 사용\n",
    "test_dir = \"fashion_mnist_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 설정 및 신경망 입력 크기\n",
    "image_size = (28, 28)  # Fashion MNIST는 28x28 크기\n",
    "input_size = image_size[0] * image_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 정의\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1024, activation='tanh', input_shape=(input_size,), kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Dense(units=10, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# CNN 모델 정의\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Layer: 32개의 필터, 커널 크기 3x3, 활성화 함수 ReLU\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Max Pooling Layer: 2x2 풀링\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Convolutional Layer: 64개의 필터, 커널 크기 3x3, 활성화 함수 ReLU\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "    # Max Pooling Layer: 2x2 풀링\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer: 2D 데이터를 1D 벡터로 변환\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer: 128 뉴런, 활성화 함수 ReLU\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "    # Output Layer: 10개의 클래스, 활성화 함수 Softmax\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def load_data(data_dir, image_size=(28, 28)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # 디렉토리 구조 탐색\n",
    "    for class_name in sorted(os.listdir(data_dir)):  # 클래스는 디렉토리 이름\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):  # 디렉토리가 아니면 건너뜀\n",
    "            continue\n",
    "        \n",
    "        # 각 클래스의 이미지 탐색\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if img_path.endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 처리\n",
    "                # 이미지 로드 및 전처리\n",
    "                img = Image.open(img_path).convert('L')  # 그레이스케일 변환\n",
    "                img = img.resize(image_size)  # 크기 조정\n",
    "                img = np.array(img) / 255.0  # 정규화 (0~1)\n",
    "                images.append(img.flatten())  # 플래튼 (784,)\n",
    "                labels.append(class_name)  # 라벨 추가\n",
    "    \n",
    "    # NumPy 배열로 변환\n",
    "    images = np.array(images, dtype=np.float32) \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # 라벨을 원-핫 인코딩\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train_orig, y_train_orig = load_data(original_train_dir)\n",
    "X_train_aug, y_train_aug = load_data(augmented_train_dir)\n",
    "X_train_combined = np.concatenate([X_train_orig, X_train_aug], axis=0)\n",
    "y_train_combined = np.concatenate([y_train_orig, y_train_aug], axis=0)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test, description):\n",
    "    print(f\"\\nTraining with {description} data...\\n\")\n",
    "\n",
    "    # CNN 모델 생성\n",
    "    model = build_cnn_model()\n",
    "\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=10,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Test 데이터 성능 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy ({description}): {test_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_model(X_train, y_train, X_valid, y_valid, X_test, y_test, description):\n",
    "    print(f\"\\nTraining with {description} data...\\n\")\n",
    "    model = build_model()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=10,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Test 데이터 성능 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy ({description}): {test_accuracy * 100:.2f}%\\n\")\n",
    "    '''\n",
    "    # Classification Report 출력\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    print(f\"Classification Report ({description}):\\n\")\n",
    "    print(classification_report(y_test_classes, y_pred_classes))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 전 학습 및 평가\n",
    "train_and_evaluate_model(X_train_orig, y_train_orig, X_valid, y_valid, X_test, y_test, \"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 후 학습 및 평가\n",
    "train_and_evaluate_model(X_train_combined, y_train_combined, X_valid, y_valid, X_test, y_test, \"Augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **개별 이미지에 대한 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def predict_single_image(model, image_path):\n",
    "    # 이미지 로드 및 전처리\n",
    "    img = Image.open(image_path).convert('L')  # 흑백 변환\n",
    "    img = img.resize((28, 28))  # 크기 조정\n",
    "    img_array = np.array(img) / 255.0  # 정규화\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)  # (1, 높이, 너비, 채널)\n",
    "\n",
    "    # 예측 수행\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)  # 가장 높은 확률의 클래스\n",
    "    confidence = np.max(predictions)  # 예측 확률\n",
    "\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 경로 설정\n",
    "image_path = \"path_to_image.png\"  # 예측할 이미지 파일 경로\n",
    "\n",
    "# CNN 모델 생성 및 학습 (이미 학습된 모델을 불러온 경우 이 단계는 생략)\n",
    "model = build_cnn_model()\n",
    "\n",
    "# 개별 이미지 예측\n",
    "predicted_class, confidence = predict_single_image(model, image_path)\n",
    "print(f\"Predicted Class: {predicted_class}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# MNIST 읽어 와서 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784) # 텐서 모양 변환\n",
    "x_test = x_test.reshape(10000,784)\n",
    "x_train=x_train.astype(np.float32)/255.0 # ndarray로 변환\n",
    "x_test=x_test.astype(np.float32)/255.0\n",
    "y_train=tf.keras.utils.to_categorical(y_train,10) # 원핫 코드로 변환\n",
    "y_test=tf.keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "n_input=784\n",
    "n_hidden=1024\n",
    "n_output=10\n",
    "\n",
    "mlp=Sequential()\n",
    "mlp.add (Dense(units=n_hidden,activation='tanh',input_shape=(n_input,),kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "mlp.add(Dense(units=n_output,activation='tanh',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "hist=mlp.fit(x_train,y_train,batch_size=128,epochs=30,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "res=mlp.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"정확률은\",res[1]*100)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 정확률 곡선\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 손실 함수 곡선\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'], loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-3. 과적합 해결방안: 모델 복잡도 낮춤**\n",
    "* 모델이 훈련 데이터의 노이즈까지 학습하게 되어 테스트 데이터나 새로운 데이터에 대한 일반화 성능 감소 \n",
    "* 인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# CIFAR-10 데이터셋을 읽어와서 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.reshape(50000, 32*32*3)  # 1차원 텐서로 변환 (50000, 3072)\n",
    "x_test = x_test.reshape(10000, 32*32*3)    # 1차원 텐서로 변환 (10000, 3072)\n",
    "x_train = x_train.astype(np.float32) / 255.0  # 0-1 범위로 정규화\n",
    "x_test = x_test.astype(np.float32) / 255.0    # 0-1 범위로 정규화\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)  # 원-핫 인코딩\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)    # 원-핫 인코딩\n",
    "\n",
    "n_input = 32*32*3  # CIFAR-10 이미지의 픽셀 수 (3072)\n",
    "n_hidden = 1024    # 은닉층 유닛 수\n",
    "n_output = 10      # 출력층 유닛 수 (10개의 클래스)\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(units=n_hidden, activation='tanh', input_shape=(n_input,), kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "mlp.add(Dense(units=n_output, activation='tanh', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "\n",
    "mlp.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "hist = mlp.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "res = mlp.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"정확률은\", res[1] * 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 정확률 곡선\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 손실 함수 곡선\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-4. 과적합 해결방안: 드롭아웃(Dropout)**\n",
    "* 일정 비율의 가중치를 임의로 선택하여 불능으로 만들고 학습하는 규제 기법\n",
    "* 불능이 될 엣지는 샘플마다 독립적으로 난수를 이용하여 랜덤하게 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/dropout.png\" alt=\" Dropout\" width=\"600\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# CIFAR-10 데이터셋을 읽고 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train=x_train.astype(np.float32)/255.0\n",
    "x_test=x_test.astype(np.float32)/255.0\n",
    "y_train=tf.keras.utils.to_categorical(y_train,10)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "# 하이퍼 매개변수 설정\n",
    "batch_siz=128\n",
    "n_epoch=10\n",
    "k=5 # k-겹 교차 검증\n",
    "\n",
    "# 드롭아웃 비율에 따라 교차 검증을 수행하고 정확률을 반환하는 함수\n",
    "def cross_validation(dropout_rate):\n",
    "    accuracy=[]\n",
    "    for train_index,val_index in KFold(k).split(x_train):\n",
    "        # 훈련 집합과 검증 집합으로 분할\n",
    "        xtrain,xval=x_train[train_index],x_train[val_index]\n",
    "        ytrain,yval=y_train[train_index],y_train[val_index]\n",
    "\n",
    "        # 신경망 모델 설계\n",
    "        cnn=Sequential()\n",
    "        cnn.add(Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "        cnn.add(Conv2D(32,(3,3),activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        cnn.add(Dropout(dropout_rate[0]))\n",
    "        cnn.add(Conv2D(64,(3,3),activation='relu'))\n",
    "        cnn.add(Conv2D(64,(3,3),activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        cnn.add(Dropout(dropout_rate[1]))\n",
    "        cnn.add(Flatten())\n",
    "        cnn.add(Dense(512,activation='relu'))\n",
    "        cnn.add(Dropout(dropout_rate[2]))\n",
    "        cnn.add(Dense(10,activation='softmax'))\n",
    "\n",
    "        # 신경망 모델을 학습하고 평가하기\n",
    "        cnn.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "        cnn.fit(xtrain,ytrain,batch_size=batch_siz,epochs=n_epoch,verbose=0)\n",
    "        accuracy.append(cnn.evaluate(xval,yval,verbose=0)[1])\n",
    "    return accuracy\n",
    "\n",
    "# 드롭아웃 비율을 달리하며 신경망을 평가\n",
    "acc_without_dropout=cross_validation([0.0,0.0,0.0])\n",
    "acc_with_dropout=cross_validation([0.25,0.25,0.5])\n",
    "\n",
    "print(\"드롭아웃 적용 안 할 때:\",np.array(acc_without_dropout).mean())\n",
    "print(\"드롭아웃 적용할 때:\",np.array(acc_with_dropout).mean())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 박스 플롯으로 정확률 표시\n",
    "plt.grid()\n",
    "plt.boxplot([acc_without_dropout,acc_with_dropout],labels=[\"Without Dropout\",\"With Dropout\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-5. 과적합 해결방안: 조기 종료(Early Stop)**\n",
    "* 모델이 훈련 데이터에서 너무 오랜 시간 동안 학습하면, 학습 데이터에 과적합되어 테스트 데이터나 새로운 데이터에 대한 일반화 성능 감소\n",
    "* 모델이 과적합되기 전에 학습을 멈추는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/Early_stop.png\" alt=\"Early Stop\" width=\"400\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 00:59:35.813182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 00:59:36.812928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 00:59:39.906802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:39.943576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:39.943643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:39.946339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:39.946413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:39.946442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:40.166146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:40.166234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:40.166244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-15 00:59:40.166279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-15 00:59:40.166341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29711/3023060504.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = cnn.fit_generator(generator.flow(x_train, y_train, batch_size=batch_siz),\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# CIFAR-10 데이터셋을 읽고 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 신경망 모델 설계\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(512, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 신경망 모델 학습(영상 증대기 활용) 및 EarlyStopping 적용\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "batch_siz = 128\n",
    "generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "# EarlyStopping 콜백 설정: 검증 손실이 5번의 에포크 동안 개선되지 않으면 학습 중단\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "hist = cnn.fit_generator(generator.flow(x_train, y_train, batch_size=batch_siz),\n",
    "                         epochs=50, validation_data=(x_test, y_test),\n",
    "                         verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# 신경망 모델 정확률 평가\n",
    "res = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"정확률은\", res[1] * 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 정확률 그래프\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 손실 함수 그래프\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-6. 과적합 해결방안: 가중치 규제(Regularization)**\n",
    "* 모델의 과적합을 방지하고 일반화 성능을 향상시키기 위해 가중치의 크기를 제한\n",
    "* 모델이 복잡해지는 것을 억제하고, 과적합 감소시킴\n",
    "* L1 정규화(L1 Regularization): 가중치의 절대값 합을 최소화\n",
    "* L2 정규화(L2 Regularization): 가중치의 제곱합을 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/l1l2.png\" alt=\"Early Stop\" width=\"400\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# CIFAR-10 데이터셋을 읽고 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 신경망 모델 설계 (L1 정규화 적용)\n",
    "cnn_l1 = Sequential()\n",
    "cnn_l1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=l1(0.01)))\n",
    "cnn_l1.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l1(0.01)))\n",
    "cnn_l1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_l1.add(Dropout(0.25))\n",
    "cnn_l1.add(Flatten())\n",
    "cnn_l1.add(Dense(128, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "cnn_l1.add(Dropout(0.5))\n",
    "cnn_l1.add(Dense(10, activation='softmax', kernel_regularizer=l1(0.01)))\n",
    "\n",
    "# 신경망 모델 학습 (L1 정규화 적용)\n",
    "cnn_l1.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "hist_l1 = cnn_l1.fit(x_train, y_train, batch_size=128, epochs=12, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# 신경망 모델 설계 (L2 정규화 적용)\n",
    "cnn_l2 = Sequential()\n",
    "cnn_l2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=l2(0.01)))\n",
    "cnn_l2.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))\n",
    "cnn_l2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_l2.add(Dropout(0.25))\n",
    "cnn_l2.add(Flatten())\n",
    "cnn_l2.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "cnn_l2.add(Dropout(0.5))\n",
    "cnn_l2.add(Dense(10, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# 신경망 모델 학습 (L2 정규화 적용)\n",
    "cnn_l2.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "hist_l2 = cnn_l2.fit(x_train, y_train, batch_size=128, epochs=12, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# 신경망 모델 정확률 평가\n",
    "res_l1 = cnn_l1.evaluate(x_test, y_test, verbose=0)\n",
    "res_l2 = cnn_l2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"L1 정규화 적용 모델 정확률은\", res_l1[1] * 100)\n",
    "print(\"L2 정규화 적용 모델 정확률은\", res_l2[1] * 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 정확률 그래프\n",
    "plt.plot(hist_l1.history['accuracy'], label='Train L1')\n",
    "plt.plot(hist_l1.history['val_accuracy'], label='Validation L1')\n",
    "plt.plot(hist_l2.history['accuracy'], label='Train L2')\n",
    "plt.plot(hist_l2.history['val_accuracy'], label='Validation L2')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 손실 함수 그래프\n",
    "plt.plot(hist_l1.history['loss'], label='Train L1')\n",
    "plt.plot(hist_l1.history['val_loss'], label='Validation L1')\n",
    "plt.plot(hist_l2.history['loss'], label='Train L2')\n",
    "plt.plot(hist_l2.history['val_loss'], label='Validation L2')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 딥러닝 최적 파라미터 (Hyperparameter)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-1. 학습률 (Hyperparameter)**\n",
    "- 학습률 값은 적절히 지정해야 함\n",
    "- 너무 크면 발산하고, 너무 작으면 학습이 잘 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1). 학습률별 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "  return x**2\n",
    "def df_dx1(x):\n",
    "  return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent2(f,df_dx, init_x, learning_rate=0.01, step_num=100):\n",
    "    eps=1e-5\n",
    "    count=0\n",
    "    \n",
    "    old_x=init_x\n",
    "    min_x=old_x\n",
    "    min_y=f(min_x)\n",
    "    \n",
    "    x_log, y_log=[min_x], [min_y]\n",
    "    for i in range(step_num):\n",
    "        grad=df_dx(old_x)\n",
    "        new_x=old_x-learning_rate*grad\n",
    "        new_y=f(new_x)\n",
    "        \n",
    "        if min_y>new_y:\n",
    "            min_x=new_x\n",
    "            min_y=new_y\n",
    "        if np.abs(old_x-new_x) <eps:\n",
    "            break\n",
    "        \n",
    "        x_log.append(old_x)\n",
    "        y_log.append(new_y)\n",
    "        \n",
    "        old_x=new_x\n",
    "        count+=1\n",
    "        \n",
    "    return x_log, y_log, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list=[0.001, 0.01, 0.1, 1.01]\n",
    "init_x=30.0\n",
    "x=np.arange(-30, 50,0.01)\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    x_log, y_log, count=gradient_descent2(f1, df_dx1,init_x=init_x, learning_rate=lr)\n",
    "    ax=fig.add_subplot(2,2,i+1)\n",
    "    ax.scatter(init_x, f1(init_x), color='green')\n",
    "    ax.plot(x_log, y_log, color='red', linewidth='4')\n",
    "    ax.plot(x, f1(x), '--')\n",
    "    ax.grid()\n",
    "    ax.title.set_text('learning rate= []'.format(str(lr)))\n",
    "    print('init value = {}, count= {}'.format(str(lr), str(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-2. 확률적 경사하강법 (Stochastic Gradient Descent)**\n",
    "* 경사하강법(Gradient Descent Algorithm)\n",
    "    * 전체 데이터를 이용하여 경사를 구하기 때문에 최저점 수렴이 안정적\n",
    "    * 전체 데이터를 모두 한 번에 처리하기 때문에 속도가 느리고 메모리가 많이 필요\n",
    "* 확률적 경사하강법(Stochastic Gradient Descent)\n",
    "    * 전체 데이터 중 랜덤하게 선택된 하나의 데이터를 이용하여 진행\n",
    "    * 적은 데이터로 학습할 수 있고 최적화 속도가 빠름\n",
    "    * 하나의 데이터를 이용하기 때문에 기울기의 방향이 크게 바뀌고 오차율이 높아지므로 최저점 안착이 비교적 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/gd_sgd.png\" alt=\"Early Stop\" width=\"800\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sgd:\n",
    "    \"\"\" SGD: Stochastic Gradient Descent\n",
    "    W = W - lr * dL/dW\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, params, gradients):\n",
    "        for key in params:\n",
    "            # W = W - lr * dl/dW\n",
    "            params[key] -= self.learning_rate * gradients[key]\n",
    "\n",
    "def fn(x, y):\n",
    "    \"\"\"f(x, y) = (1/20) * x**2 + y**2\"\"\"\n",
    "    return x**2 / 20 + y**2\n",
    "\n",
    "def fn_derivative(x, y): # 함수 fn의 미분값\n",
    "    return x/10, 2*y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Sgd 클래스의 객체(인스턴스)를 생성\n",
    "    sgd = Sgd(0.95)\n",
    "\n",
    "    # ex01 모듈에서 작성한 fn(x, y) 함수의 최솟값을 임의의 점에서 시작해서 찾아감.\n",
    "    init_position = (-7, 2)\n",
    "\n",
    "    # 신경망에서 찾고자 하는 파라미터의 초깃값\n",
    "    params = dict()\n",
    "    params['x'], params['y'] = init_position[0], init_position[1]\n",
    "\n",
    "    # 각 파라미터에 대한 변화율(gradient)\n",
    "    gradients = dict()\n",
    "    gradients['x'], gradients['y'] = 0, 0\n",
    "\n",
    "    # 각 파라미터들(x, y)을 갱신할 때마다 갱신된 값을 저장할 리스트\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    for i in range(30):\n",
    "        x_history.append(params['x'])\n",
    "        y_history.append(params['y'])\n",
    "        gradients['x'], gradients['y'] = fn_derivative(params['x'], params['y'])  # gradients 갱신\n",
    "        sgd.update(params, gradients)\n",
    "\n",
    "    for x, y in zip(x_history, y_history):\n",
    "        print(f'({x}, {y})')\n",
    "\n",
    "    x = np.linspace(-10, 10, 200)\n",
    "    y = np.linspace(-5, 5, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = fn(X, Y)\n",
    "\n",
    "    plt.contour(X, Y, Z, 30)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.axis('equal')\n",
    "\n",
    "    # 등고선 그래프에 파라미터(x, y)들이 갱신되는 과정을 추가.\n",
    "    plt.plot(x_history, y_history, 'o-', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. 모멘텀 (Momentom)**\n",
    "* 이전 방향 정보를 같이 고려하여 학습\n",
    "* Saddle point를 만나거나 로컬 미니멈(Local Minimum)에 빠지는 위험 감소\n",
    "* 네스테토프 모멘텀(Nesterov): 현재의 위치뿐만 아니라 예측된 다음 위치에서의 기울기를 사용하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/momentum_math.png\" alt=\"Early Stop\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/momentum.png\" alt=\"Early Stop\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모멘텀과 네스테토프 모멘텀 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-4. 하이퍼 파라미터 최적화 예제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True, name='SGD') # 모멘텀 최적화 알고리즘 설정\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-5. 적응적 학습률 (Momentom)**\n",
    "* 그레디언트는 최저점의 방향은 알려주지만, 이동량에 대한 정보는 없기 때문에 작은 학습률을 곱해 조금씩 보수적으로 이동\n",
    "* 학습률이 너무 작으면 학습에 많은 시간 소요, 너무 크면 진동\n",
    "* 사황에 맞게 학습률을 조절하는 방법\n",
    "* Adagrad(Adaptive Gradient): 이전 그레디언트를 누적한 정보를 이용하여 학습률을 적응적으로 설정\n",
    "* RMSProp(Root Mean Square + Propagation): 이전 그레디언트를 누적할 때 오래된 것의 영향을 줄이는 정책을 사용하여 AdaGrad를 개선한 기법\n",
    "* Adam(Adaptive Learning + Momentum): RMSProp에 모멘텀을 적용한 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD,Adam,Adagrad,RMSprop\n",
    "\n",
    "# fashion MNIST 읽어 와서 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)\n",
    "x_train=x_train.astype(np.float32)/255.0\n",
    "x_test=x_test.astype(np.float32)/255.0\n",
    "y_train=tf.keras.utils.to_categorical(y_train,10)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "# 신경망 구조 설정\n",
    "n_input=784\n",
    "n_hidden1=1024\n",
    "n_hidden2=512\n",
    "n_hidden3=512\n",
    "n_hidden4=512\n",
    "n_output=10\n",
    "\n",
    "# 하이퍼 매개변수 설정\n",
    "batch_siz=256\n",
    "n_epoch=50\n",
    "\n",
    "# 모델을 설계해주는 함수(모델을 나타내는 객체 model을 반환)\n",
    "def build_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=n_hidden1,activation='relu',input_shape=(n_input,)))\n",
    "    model.add(Dense(units=n_hidden2,activation='relu'))\n",
    "    model.add(Dense(units=n_hidden3,activation='relu'))\n",
    "    model.add(Dense(units=n_hidden4,activation='relu'))\n",
    "    model.add(Dense(units=n_output,activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# SGD 옵티마이저를 사용하는 모델\n",
    "dmlp_sgd=build_model()\n",
    "dmlp_sgd.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
    "hist_sgd=dmlp_sgd.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# Adam 옵티마이저를 사용하는 모델\n",
    "dmlp_adam=build_model()\n",
    "dmlp_adam.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "hist_adam=dmlp_adam.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# Adagrad 옵티마이저를 사용하는 모델\n",
    "dmlp_adagrad=build_model()\n",
    "dmlp_adagrad.compile(loss='categorical_crossentropy',optimizer=Adagrad(),metrics=['accuracy'])\n",
    "hist_adagrad=dmlp_adagrad.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# RMSprop 옵티마이저를 사용하는 모델\n",
    "dmlp_rmsprop=build_model()\n",
    "dmlp_rmsprop.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "hist_rmsprop=dmlp_rmsprop.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# 네 모델의 정확률을 출력\n",
    "print(\"SGD 정확률은\",dmlp_sgd.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"Adam 정확률은\",dmlp_adam.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"Adagrad 정확률은\",dmlp_adagrad.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"RMSprop 정확률은\",dmlp_rmsprop.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 네 모델의 정확률을 하나의 그래프에서 비교\n",
    "plt.plot(hist_sgd.history['accuracy'],'r')\n",
    "plt.plot(hist_sgd.history['val_accuracy'],'r--')\n",
    "plt.plot(hist_adam.history['accuracy'],'g')\n",
    "plt.plot(hist_adam.history['val_accuracy'],'g--')\n",
    "plt.plot(hist_adagrad.history['accuracy'],'b')\n",
    "plt.plot(hist_adagrad.history['val_accuracy'],'b--')\n",
    "plt.plot(hist_rmsprop.history['accuracy'],'m')\n",
    "plt.plot(hist_rmsprop.history['val_accuracy'],'m--')\n",
    "plt.title('Model accuracy comparison between optimizers')\n",
    "plt.ylim((0.6,1.0))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_sgd','Val_sgd','Train_adam','Val_adam','Train_adagrad','Val_adagrad','Train_rmsprop','Val_rmsprop'], loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion MNIST 읽어 와서 신경망에 입력할 형태로 변환\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)\n",
    "x_train=x_train.astype(np.float32)/255.0\n",
    "x_test=x_test.astype(np.float32)/255.0\n",
    "y_train=tf.keras.utils.to_categorical(y_train,10)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "# 신경망 구조 설정\n",
    "n_input=784\n",
    "n_hidden1=1024\n",
    "n_hidden2=512\n",
    "n_hidden3=512\n",
    "n_hidden4=512\n",
    "n_output=10\n",
    "\n",
    "# 하이퍼 매개변수 설정\n",
    "batch_siz=256\n",
    "n_epoch=50\n",
    "\n",
    "# 모델을 설계해주는 함수(모델을 나타내는 객체 model을 반환)\n",
    "def build_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=n_hidden1,activation='relu',input_shape=(n_input,)))\n",
    "    model.add(Dense(units=n_hidden2,activation='relu'))\n",
    "    model.add(Dense(units=n_hidden3,activation='relu'))\n",
    "    model.add(Dense(units=n_hidden4,activation='relu'))\n",
    "    model.add(Dense(units=n_output,activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# SGD 옵티마이저를 사용하는 모델\n",
    "dmlp_sgd=build_model()\n",
    "dmlp_sgd.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
    "hist_sgd=dmlp_sgd.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# Adam 옵티마이저를 사용하는 모델\n",
    "dmlp_adam=build_model()\n",
    "dmlp_adam.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "hist_adam=dmlp_adam.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# Adagrad 옵티마이저를 사용하는 모델\n",
    "dmlp_adagrad=build_model()\n",
    "dmlp_adagrad.compile(loss='categorical_crossentropy',optimizer=Adagrad(),metrics=['accuracy'])\n",
    "hist_adagrad=dmlp_adagrad.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# RMSprop 옵티마이저를 사용하는 모델\n",
    "dmlp_rmsprop=build_model()\n",
    "dmlp_rmsprop.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "hist_rmsprop=dmlp_rmsprop.fit(x_train,y_train,batch_size=batch_siz,epochs=n_epoch,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# 네 모델의 정확률을 출력\n",
    "print(\"SGD 정확률은\",dmlp_sgd.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"Adam 정확률은\",dmlp_adam.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"Adagrad 정확률은\",dmlp_adagrad.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "print(\"RMSprop 정확률은\",dmlp_rmsprop.evaluate(x_test,y_test,verbose=0)[1]*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 네 모델의 정확률을 하나의 그래프에서 비교\n",
    "plt.plot(hist_sgd.history['accuracy'],'r')\n",
    "plt.plot(hist_sgd.history['val_accuracy'],'r--')\n",
    "plt.plot(hist_adam.history['accuracy'],'g')\n",
    "plt.plot(hist_adam.history['val_accuracy'],'g--')\n",
    "plt.plot(hist_adagrad.history['accuracy'],'b')\n",
    "plt.plot(hist_adagrad.history['val_accuracy'],'b--')\n",
    "plt.plot(hist_rmsprop.history['accuracy'],'m')\n",
    "plt.plot(hist_rmsprop.history['val_accuracy'],'m--')\n",
    "plt.title('Model accuracy comparison between optimizers')\n",
    "plt.ylim((0.6,1.0))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_sgd','Val_sgd','Train_adam','Val_adam','Train_adagrad','Val_adagrad','Train_rmsprop','Val_rmsprop'], loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 하이퍼 파라미터 튜닝(Hyperparameter Tuning)**\n",
    "* 머신러닝 모델의 성능을 최적화하기 위해 모델 학습 과정에서 고정된 값을 가지는 하이퍼파라미터를 조정하는 과정\n",
    "* 주요 하이퍼파라미터로는 학습률(learning rate), 정규화 파라미터, 은닉층의 수와 크기 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-1. 그리드 서치(Grid Search)**\n",
    "* 그리드 서치는 사전에 정의된 하이퍼파라미터 값의 조합을 모두 탐색하여 최적의 파라미터를 찾는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Grid_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define Keras model function\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential([...])  # Define model architecture\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search_result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-2. 랜덤 서치(Randomized Search)**\n",
    "* 하이퍼파라미터 공간에서 무작위로 일부 조합을 선택하여 탐색하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Random_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, cv=3, n_iter=5)\n",
    "random_search_result = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. 베이지안 최적화(Bayesian Optimization)**\n",
    "* 이전 탐색 결과를 바탕으로, 다음 탐색할 하이퍼파라미터 조합을 선택하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Bayesian_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(estimator=model, search_spaces=param_space, cv=3, n_iter=5)\n",
    "bayes_search_result = bayes_search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "day16_1_jupyter",
   "language": "python",
   "name": "day16_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
