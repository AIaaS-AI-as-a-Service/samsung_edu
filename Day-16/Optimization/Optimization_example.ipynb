{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **최적화(Optimization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 목적함수(Object Function)을 최대/최소화 하는 파라미터 조합 탐색 문제\n",
    "* 머신 러닝에서는 일반적으로 모델이 데이터를 잘 예측하거나 분류할 수 있도록 손실 함수(loss function)를 최소화하는 것이 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 사용 확인\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. 데이터 준비**\n",
    "* Fashion MNIST\n",
    "* 10개의 의류 품목(셔츠, 바지 등)으로 구성된 28x28 픽셀의 흑백 이미지 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터셋 경로 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_dir = \"fashion_mnist_images\"  \n",
    "output_base_dir = \"fashion_mnist_dataset\"\n",
    "train_dir = os.path.join(output_base_dir, \"train\")\n",
    "valid_dir = os.path.join(output_base_dir, \"valid\")\n",
    "test_dir = os.path.join(output_base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in os.listdir(original_data_dir):\n",
    "    print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터셋 분할: 0.7 (Train), 0.2 (Validation), 0.1 (Test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_data_stratified(original_dir, train_ratio=0.7, valid_ratio=0.2):\n",
    "    # 모든 데이터를 모으고 라벨 생성\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(original_dir):\n",
    "        class_dir = os.path.join(original_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if img_name.endswith(('.png', '.jpg')):\n",
    "                image_paths.append(os.path.join(class_dir, img_name))\n",
    "                labels.append(class_name)\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "        image_paths, labels, test_size=(1 - train_ratio), stratify=labels, random_state=42\n",
    "    )\n",
    "    valid_size = valid_ratio / (1 - train_ratio)  # validation 비율 조정\n",
    "    valid_images, test_images, valid_labels, test_labels = train_test_split(\n",
    "        temp_images, temp_labels, test_size=(1 - valid_size), stratify=temp_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # 데이터를 출력 디렉토리에 저장\n",
    "    for dataset, output_dir in zip(\n",
    "        [(train_images, train_labels), (valid_images, valid_labels), (test_images, test_labels)],\n",
    "        [train_dir, valid_dir, test_dir]\n",
    "    ):\n",
    "        images, labels = dataset\n",
    "        for img_path, label in zip(images, labels):\n",
    "            class_output_dir = os.path.join(output_dir, label)\n",
    "            os.makedirs(class_output_dir, exist_ok=True)\n",
    "            shutil.copy(img_path, class_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_stratified(original_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 과적합(Overfitting)**\n",
    "* 모델이 학습 데이터에 대한 성능은 매우 우수하지만, 새로운 데이터에 대해서는 일반화 성능이 떨어지는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/Fitting.png\" alt=\" Frtting\" width=\"1000\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-2. 과적합 해결방안: 데이터 증강(Data Augmentation)**\n",
    "* 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 학습\n",
    "* 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합 방지\n",
    "* Train Data에 대해서만 데이터 증강 수행(Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 증강(Data Augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgaug의 iaa.Sequential을 사용하여 다양한 증강 기법을 적용\n",
    "def augment_images(input_dir, output_dir):\n",
    "    \n",
    "    # Augmentation 설정\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),  # 수평 반전\n",
    "        #iaa.Flipud(0.5),  # 수직 반전\n",
    "        iaa.Affine(rotate=(-15, 15)),  # -15도에서 15도까지 회전\n",
    "        #iaa.Multiply((0.8, 1.2)),  # 밝기 조정\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0))  # 가우시안 Blur\n",
    "    ])\n",
    "\n",
    "    class_dirs = [os.path.join(input_dir, class_name) for class_name in os.listdir(input_dir)]\n",
    "    for class_dir in class_dirs:\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        output_class_dir = os.path.join(output_dir, os.path.basename(class_dir))\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        # 이미지를 읽어서 증강 후 저장\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if not img_path.endswith(('.png', '.jpg')):\n",
    "                continue\n",
    "\n",
    "            # 이미지를 불러와 증강\n",
    "            image = Image.open(img_path)\n",
    "            image_np = np.array(image)\n",
    "            augmented_images = seq(images=[image_np] * 1)  # 이미지 1개 생성\n",
    "\n",
    "            for i, aug_img in enumerate(augmented_images):\n",
    "                aug_img = Image.fromarray(aug_img)\n",
    "                aug_img.save(os.path.join(output_class_dir, f\"{img_name.split('.')[0]}_aug_{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train='fashion_mnist_dataset/train'\n",
    "output_train='augmented_fashion_mnist_dataset/train'\n",
    "augment_images(input_train, output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **증강된 이미지 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 증강된 데이터 경로 설정\n",
    "augmented_data_dir = \"augmented_fashion_mnist_dataset/train\"  # 증강 데이터가 저장된 디렉토리\n",
    "class_name = \"Ankle boot\"  # 확인할 클래스 이름\n",
    "class_dir = os.path.join(augmented_data_dir, class_name)\n",
    "\n",
    "# 특정 클래스 디렉토리에서 이미지 읽기\n",
    "images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('.png', '.jpg'))]\n",
    "\n",
    "# 확인할 이미지 선택\n",
    "num_images_to_display = 5  # 확인할 이미지 수\n",
    "selected_images = images[:num_images_to_display]\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, img_path in enumerate(selected_images):\n",
    "    # 파일 이름 추출\n",
    "    image_name = os.path.basename(img_path)\n",
    "    \n",
    "    # PIL로 이미지 열기\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.subplot(1, num_images_to_display, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(image_name)  # 이미지 파일 이름을 제목으로 표시\n",
    "    \n",
    "    # 파일 이름 출력\n",
    "    print(f\"Displaying: {image_name}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **증강 전후 결과 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "original_train_dir = \"fashion_mnist_dataset/train\"\n",
    "augmented_train_dir = \"augmented_fashion_mnist_dataset/train\"\n",
    "valid_dir = \"fashion_mnist_dataset/valid\"  # 증강 후에도 동일한 validation 사용\n",
    "test_dir = \"fashion_mnist_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 설정 및 신경망 입력 크기\n",
    "image_size = (28, 28)  # Fashion MNIST는 28x28 크기\n",
    "input_size = image_size[0] * image_size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN 모델 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# CNN 모델 정의\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) # Convolutional Layer: 32개의 필터, 커널 크기 3x3, 활성화 함수 ReLU\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))# Max Pooling Layer: 2x2 풀링\n",
    "\n",
    "    # Convolutional Layer: 64개의 필터, 커널 크기 3x3, 활성화 함수 ReLU\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) # Convolutional Layer: 64개의 필터, 커널 크기 3x3, 활성화 함수 ReLU\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))# Max Pooling Layer: 2x2 풀링\n",
    "    model.add(Flatten()) # Flatten Layer: 2D 데이터를 1D 벡터로 변환\n",
    "\n",
    "    \n",
    "    model.add(Dense(units=128, activation='relu')) # Fully Connected Layer: 128 뉴런, 활성화 함수 ReLU\n",
    "    model.add(Dense(units=10, activation='softmax')) # Output Layer: 10개의 클래스, 활성화 함수 Softmax\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_data(data_dir, image_size=(28, 28)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # 디렉토리 구조 탐색\n",
    "    for class_name in sorted(os.listdir(data_dir)):  # 클래스는 디렉토리 이름\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):  # 디렉토리가 아니면 건너뜀\n",
    "            continue\n",
    "        \n",
    "        # 각 클래스의 이미지 탐색\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if img_path.endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 처리\n",
    "                # 이미지 로드 및 전처리\n",
    "                img = Image.open(img_path).convert('L')  # 그레이스케일 변환\n",
    "                img = img.resize(image_size)  # 크기 조정\n",
    "                img = np.array(img) / 255.0  # 정규화 (0~1)\n",
    "                images.append(img.flatten())  # 플래튼 (784,)\n",
    "                labels.append(class_name)  # 라벨 추가\n",
    "    \n",
    "    # NumPy 배열로 변환\n",
    "    images = np.array(images, dtype=np.float32) \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # 라벨을 정수형으로 변환\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)  # 문자열 라벨 → 정수 라벨로 변환\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train, y_train = load_data(original_train_dir)\n",
    "X_train_aug, y_train_aug = load_data(augmented_train_dir)\n",
    "X_train_combined = np.concatenate([X_train, X_train_aug], axis=0)\n",
    "y_train_combined = np.concatenate([y_train, y_train_aug], axis=0)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN 모델 학습 및 평가 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model, epochs, description):\n",
    "    print(f\"\\nTraining with {description} data...\\n\")\n",
    "\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Test 데이터 성능 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy ({description}): {test_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **학습 결과 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, description):\n",
    "    # 학습 및 검증 손실 곡선\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{description} - Loss')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 학습 및 검증 정확도 곡선\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{description} - Accuracy')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **원본 데이터만 학습한 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "model=build_cnn_model()\n",
    "history_orig = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model, 10, \"Original\")\n",
    "plot_training_history(history_orig, \"Original Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **증강된 데이터를 포함해 학습한 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=build_cnn_model()\n",
    "history_aug = train_and_evaluate_cnn_model(X_train_combined, y_train_combined, X_valid, y_valid, X_test, y_test, model, 10, \"Augmented\")\n",
    "plot_training_history(history_aug, \"Augmented Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-3. 과적합 해결방안: 모델 복잡도 낮춤**\n",
    "* 모델이 훈련 데이터의 노이즈까지 학습하게 되어 테스트 데이터나 새로운 데이터에 대한 일반화 성능 감소 \n",
    "* 인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-4. 과적합 해결방안: 드롭아웃(Dropout)**\n",
    "* 일정 비율의 가중치를 임의로 선택하여 불능으로 만들고 학습하는 규제 기법\n",
    "* 불능이 될 엣지는 샘플마다 독립적으로 난수를 이용하여 랜덤하게 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/dropout.png\" alt=\" Dropout\" width=\"600\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout 적용하지 않은 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_no_dropout(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout 적용한 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# CNN 모델 정의\n",
    "def build_cnn_model_dropout(input_shape=(28, 28, 1), num_classes=10, dropout_rate=[0.25, 0.25, 0.5]):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=dropout_rate[0]))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=dropout_rate[1]))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate[2]))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 비교 평가 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout 적용 전후 비교 학습\n",
    "def compare_dropout_models(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    # Dropout 없는 모델\n",
    "    print(\"\\nTraining CNN Model WITHOUT Dropout...\\n\")\n",
    "    no_dropout_model = build_cnn_model_no_dropout()\n",
    "    X_train_no_dropout = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_valid_no_dropout = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test_no_dropout = X_test.reshape(-1, 28, 28, 1)\n",
    "    history_no_dropout = no_dropout_model.fit(\n",
    "        X_train_no_dropout, y_train,\n",
    "        validation_data=(X_valid_no_dropout, y_valid),\n",
    "        epochs=10,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss_no_dropout, test_accuracy_no_dropout = no_dropout_model.evaluate(X_test_no_dropout, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy WITHOUT Dropout: {test_accuracy_no_dropout * 100:.2f}%\\n\")\n",
    "\n",
    "    # Dropout 있는 모델\n",
    "    print(\"\\nTraining CNN Model WITH Dropout...\\n\")\n",
    "    dropout_model = build_cnn_model_dropout()\n",
    "    X_train_dropout = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_valid_dropout = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test_dropout = X_test.reshape(-1, 28, 28, 1)\n",
    "    history_dropout = dropout_model.fit(\n",
    "        X_train_dropout, y_train,\n",
    "        validation_data=(X_valid_dropout, y_valid),\n",
    "        epochs=10,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss_dropout, test_accuracy_dropout = dropout_model.evaluate(X_test_dropout, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy WITH Dropout: {test_accuracy_dropout * 100:.2f}%\\n\")\n",
    "\n",
    "    # 결과 시각화\n",
    "    print(\"Plotting results...\")\n",
    "    plot_training_history(history_no_dropout, \"CNN WITHOUT Dropout\")\n",
    "    plot_training_history(history_dropout, \"CNN WITH Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train, y_train = load_data(original_train_dir)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# Dropout 적용 전후 모델 비교\n",
    "compare_dropout_models(X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-5. 과적합 해결방안: 조기 종료(Early Stop)**\n",
    "* 모델이 훈련 데이터에서 너무 오랜 시간 동안 학습하면, 학습 데이터에 과적합되어 테스트 데이터나 새로운 데이터에 대한 일반화 성능 감소\n",
    "* 모델이 과적합되기 전에 학습을 멈추는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/Early_stop.png\" alt=\"Early Stop\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Early Stop 적용한 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_and_evaluate_cnn_model_with_early_stopping(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs, patience,description):\n",
    "    print(f\"\\nTraining with {description} data and Early Stopping...\\n\")\n",
    "\n",
    "    # CNN 모델 생성\n",
    "    model = build_cnn_model()\n",
    "\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # Early Stopping 콜백 추가\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # 검증 손실을 기준으로 관찰\n",
    "        patience=patience,          # 개선되지 않은 에포크 수 (patience 에포크 동안 개선 없으면 멈춤)\n",
    "        restore_best_weights=True  # 최적의 가중치 복원\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,  # 최대 에포크 수\n",
    "        batch_size=8,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Test 데이터 성능 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy ({description}): {test_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 비교 평가 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_early_stopping(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    # Early Stopping 없이 학습\n",
    "    print(\"Training CNN Model WITHOUT Early Stopping...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    history_no_es = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test,model, 50 ,\"CNN WITHOUT Early Stopping\")\n",
    "\n",
    "    # Early Stopping 적용 학습\n",
    "    print(\"Training CNN Model WITH Early Stopping...\\n\")\n",
    "    history_with_es = train_and_evaluate_cnn_model_with_early_stopping(X_train, y_train, X_valid, y_valid, X_test, y_test, 50, 5, \"CNN WITH Early Stopping\")\n",
    "\n",
    "    # 결과 시각화\n",
    "    print(\"Plotting results...\")\n",
    "    plot_training_history(history_no_es, \"CNN WITHOUT Early Stopping\")\n",
    "    plot_training_history(history_with_es, \"CNN WITH Early Stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train, y_train = load_data(original_train_dir)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# Early Stopping 전후 비교\n",
    "compare_early_stopping(X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-6. 과적합 해결방안: 가중치 규제(Regularization)**\n",
    "* 모델의 과적합을 방지하고 일반화 성능을 향상시키기 위해 가중치의 크기를 제한\n",
    "* 모델이 복잡해지는 것을 억제하고, 과적합 감소시킴\n",
    "* L1 정규화(L1 Regularization)\n",
    "    * 가중치의 절대값 합을 최소화\n",
    "    * 일부 가중치를 정확히 0으로 만듦\n",
    "    * 모델을 더 희소(sparse)하게 만들어 불필요한 특성을 제거\n",
    "* L2 정규화(L2 Regularization)\n",
    "    * 가중치의 제곱합을 최소화\n",
    "    * Weight Decay:딥러닝 모델의 가중치 크기를에 L2 정규화 항을 규제하여 과적합을 방지하는 정규화 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/l1l2.png\" alt=\"Early Stop\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **규제가 없는 모델** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model, epochs, description,):\n",
    "    print(f\"\\nTraining with {description} data...\\n\")\n",
    "\n",
    "\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Test 데이터 성능 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy ({description}): {test_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **L1 Regularization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "def build_cnn_model_l1(input_shape=(28, 28, 1), num_classes=10, l1_rate=0.01):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape,\n",
    "                     kernel_regularizer=l1(l1_rate)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',\n",
    "                     kernel_regularizer=l1(l1_rate)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(units=128, activation='relu', kernel_regularizer=l1(l1_rate)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **L2 Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_cnn_model_l2(input_shape=(28, 28, 1), num_classes=10, l2_rate=0.01):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape,\n",
    "                     kernel_regularizer=l2(l2_rate)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',\n",
    "                     kernel_regularizer=l2(l2_rate)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(units=128, activation='relu', kernel_regularizer=l2(l2_rate)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 비교 평가 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_regularization_models(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    # Regularization 없이 학습\n",
    "    print(\"Training CNN Model WITHOUT Regularization...\\n\")\n",
    "    model=build_cnn_model()\n",
    "    history_no_reg = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model,20,\"CNN WITHOUT Regularization\")\n",
    "\n",
    "    # L1 Regularization 적용\n",
    "    print(\"Training CNN Model WITH L1 Regularization...\\n\")\n",
    "    model_l1 = build_cnn_model_l1()\n",
    "    history_l1 = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model_l1,20,\"CNN WITH L1 Regularization\")\n",
    "\n",
    "    # L2 Regularization 적용\n",
    "    print(\"Training CNN Model WITH L2 Regularization...\\n\")\n",
    "    model_l2 = build_cnn_model_l2()\n",
    "    history_l2 = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test,model_l2,20 ,\"CNN WITH L2 Regularization\")\n",
    "\n",
    "    # 결과 시각화\n",
    "    print(\"Plotting results...\")\n",
    "    plot_training_history(history_no_reg, \"CNN WITHOUT Regularization\")\n",
    "    plot_training_history(history_l1, \"CNN WITH L1 Regularization\")\n",
    "    plot_training_history(history_l2, \"CNN WITH L2 Regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train, y_train = load_data(original_train_dir)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# Regularization 비교\n",
    "compare_regularization_models(X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-6. 과적합 해결방안: 배치 정규화(Batch Normalization)**\n",
    "* 배치(Batch):전체 데이터셋을 여러 작은 그룹으로 나눈 데이터 묶음으로, 한 번의 학습(iteration)에서 처리되는 데이터 단위\n",
    "* 학습 과정에서 각 배치 단위 별 다양한 분포를 가진 데이터를 각 배치별로 평균과 분산을 이용해 정규화\n",
    "* 모델이 특정 입력값의 스케일이나 분포에 덜 민감해지므로, 네트워크가 데이터를 지나치게 복잡하게 학습(overfitting)하는 것을 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/BatchNormalization.png\" alt=\"Early Stop\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **배치 정규화를 적용하지 않은 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Batch Normalization이 없는 CNN 모델\n",
    "def build_cnn_model_without_bn(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **배치 정규화를 적용한 모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization이 포함된 CNN 모델\n",
    "def build_cnn_model_with_bn(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 첫 번째 Convolutional Layer + Batch Normalization\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 두 번째 Convolutional Layer + Batch Normalization\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected Layer + Batch Normalization\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 비교 평가 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization 비교 함수\n",
    "def compare_batch_normalization_models(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    # Batch Normalization 없이 학습\n",
    "    print(\"Training CNN Model WITHOUT Batch Normalization...\\n\")\n",
    "    model_without_bn = build_cnn_model_without_bn()\n",
    "    history_without_bn = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model_without_bn, 10, \"CNN WITHOUT Batch Normalization\")\n",
    "\n",
    "    # Batch Normalization 포함 학습\n",
    "    print(\"Training CNN Model WITH Batch Normalization...\\n\")\n",
    "    model_with_bn = build_cnn_model_with_bn()\n",
    "    history_with_bn = train_and_evaluate_cnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, model_with_bn, 10, \"CNN WITH Batch Normalization\")\n",
    "\n",
    "    # 결과 시각화\n",
    "    print(\"Plotting results...\")\n",
    "    plot_training_history(history_without_bn, \"CNN WITHOUT Batch Normalization\")\n",
    "    plot_training_history(history_with_bn, \"CNN WITH Batch Normalization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X_train, y_train = load_data(original_train_dir)\n",
    "X_valid, y_valid = load_data(valid_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# Batch Normalization 비교\n",
    "compare_batch_normalization_models(X_train, y_train, X_valid, y_valid, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 최적화 알고리즘 (Optimizer)**\n",
    "* 딥러닝 모델의 손실 함수를 최소화하기 위해 가중치(weight)와 편향(bias)을 효율적으로 업데이트하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD,Adam,Adagrad,RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-1.확률적 경사 하강법(Stochastic Gradient Descent)**\n",
    "* 확률적으로 데이터를 뽑아 단일 데이터 포인트 또는 작은 배치를 사용하여 가중치들을 업데이트하는 방법\n",
    "* 전체 데이터를 사용하는 경사하강법과 달리 SGD는 연산량이 비교적 매우 적어 손실 함수의 최적값에 빠르게 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/SGD.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>\n",
    "<img src=\"Contents/EQ_SGD.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs):\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(\"\\nTraining with SGD...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    optimizer = SGD(learning_rate=0.01)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy (SGD): {test_accuracy * 100:.2f}%\\n\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sgd = train_with_sgd(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs)\n",
    "plot_training_history(history_sgd, \"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-2. 모멘텀(Momentum)**\n",
    "* SGD에 관성을 추가한 기법으로, 이전 단계의 기울기 방향을 일부 유지하여 진동을 줄이고 빠르게 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/Momentum.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/> \n",
    "<img src=\"Contents/EQ_Momentum.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_momentum(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs):\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(\"\\nTraining with SGD + Momentum...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy (SGD + Momentum): {test_accuracy * 100:.2f}%\\n\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_momentum = train_with_momentum(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs)\n",
    "plot_training_history(history_momentum, \"SGD with Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. 적응적 학습률(Adaptive Learning rate)**\n",
    "* 학습 과정에서 각 파라미터의 업데이트 크기를 자동으로 조정하여 학습률을 동적으로 변화시키는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2-3-1. AdaGrad(Adaptive Gradient)**\n",
    "* 각 가중치의 학습률을 개별적으로 조정하여 많이 업데이트된 가중치 학습률은 작게, 덜 업데이트된 가중치는 크게 조정하여 학습\n",
    "* 학습률이 너무 작아져 학습이 멈출 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/AdaGrad.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/> \n",
    "<img src=\"Contents/EQ_AdaGrad.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_adagrad(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs):\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(\"\\nTraining with AdaGrad...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    optimizer = Adagrad(learning_rate=0.01)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy (AdaGrad): {test_accuracy * 100:.2f}%\\n\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adagrad = train_with_adagrad(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs)\n",
    "plot_training_history(history_adagrad, \"AdaGrad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2-3-2. RMSProp(Root Mean Sqaure Propagation)**\n",
    "* AdaGrad의 문제를 개선한 방법으로, 기울기 제곱의 이동 평균을 사용하여 학습률을 조정\n",
    "* 먼 과거의 기울기는 조금 반영하고 최신의 기울기를 많이 반영\n",
    "* 학습률이 지나치게 감소하는 문제를 해결하고, 수렴 속도를 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/EQ_RMSProp.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_rmsprop(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs):\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(\"\\nTraining with RMSProp...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    optimizer = RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy (RMSProp): {test_accuracy * 100:.2f}%\\n\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rmsprop = train_with_rmsprop(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs)\n",
    "plot_training_history(history_rmsprop , \"RMSProp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2-3-3. Adam(Adaptive Moment Estimation)**\n",
    "* Momentum과 RMSProp을 결합한 최적화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Contents/Adam.png\" alt=\" Frtting\" width=\"500\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_adam(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs):\n",
    "    # 데이터 형태 변환: CNN 입력에 맞게 4D 텐서로 변환\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)  # (샘플 수, 높이, 너비, 채널 수)\n",
    "    X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(\"\\nTraining with Adam...\\n\")\n",
    "    model = build_cnn_model()\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=epochs,\n",
    "        batch_size=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy (Adam): {test_accuracy * 100:.2f}%\\n\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = train_with_adam(X_train, y_train, X_valid, y_valid, X_test, y_test, epochs)\n",
    "plot_training_history(history_rmsprop , \"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 하이퍼 파라미터 튜닝(Hyperparameter Tuning)**\n",
    "* 머신러닝 모델의 성능을 최적화하기 위해 모델 학습 과정에서 고정된 값을 가지는 하이퍼파라미터를 조정하는 과정\n",
    "* 주요 하이퍼파라미터로는 학습률(learning rate), 정규화 파라미터, 은닉층의 수와 크기 등\n",
    "* Keras-Tuner를 사용하여 딥러닝 모델 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-1. 그리드 서치(Grid Search)**\n",
    "* 그리드 서치는 사전에 정의된 하이퍼파라미터 값의 조합을 모두 탐색하여 최적의 파라미터를 찾는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Grid_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from kerastuner.tuners import GridSearch\n",
    "\n",
    "# CNN 모델 생성 함수\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters1', values=[32, 64]), \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu', \n",
    "        input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters2', values=[64, 128]), \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout1', values=[0.2, 0.3])))\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters3', values=[128, 256]), \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout2', values=[0.3, 0.4])))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Choice('dense_units', values=[128, 256]), activation='relu'))\n",
    "    model.add(Dropout(hp.Choice('dropout3', values=[0.4, 0.5])))\n",
    "    model.add(Dense(10, activation='softmax'))  # 10개의 클래스\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.01])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Keras Tuner Grid Search 설정\n",
    "tuner = GridSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # 최적화 목표\n",
    "    max_trials=20,            # 시도할 하이퍼파라미터 조합 수\n",
    "    executions_per_trial=1,   # 각 조합 실행 횟수\n",
    "    directory='my_dir',       # 결과 저장 디렉토리\n",
    "    project_name='cnn_grid_tuning' # 프로젝트 이름\n",
    ")\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_reshaped = X_train_combined.reshape(-1, 28, 28, 1)  # 4D 텐서로 변환\n",
    "X_valid_reshaped = X_valid.reshape(-1, 28, 28, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Grid Search 실행\n",
    "tuner.search(X_train_reshaped, y_train_combined, \n",
    "             validation_data=(X_valid_reshaped, y_valid), \n",
    "             epochs=10, \n",
    "             batch_size=32)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# 최적 모델로 학습 및 평가\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_reshaped, y_train_combined, \n",
    "                         validation_data=(X_valid_reshaped, y_valid), \n",
    "                         epochs=10, \n",
    "                         batch_size=32)\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-2. 랜덤 서치(Randomized Search)**\n",
    "* 하이퍼파라미터 공간에서 무작위로 일부 조합을 선택하여 탐색하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Random_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Keras Tuner에서 사용할 모델 생성 함수\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters1', values=[32, 64]),  # 'filters1'에 대해 32 또는 64 선택 #hp.Choice('name', values=[value1, value2, ...])\n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu', \n",
    "        input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters2', values=[64, 128]), # 'filters2'에 대해 64 또는 128 선택\n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout1', values=[0.2, 0.3]))) # 'dropout1'에 대해 0.2 또는 0.3 선택\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters3', values=[128, 256]), \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout2', values=[0.3, 0.4])))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Choice('dense_units', values=[128, 256]), activation='relu'))\n",
    "    model.add(Dropout(hp.Choice('dropout3', values=[0.4, 0.5])))\n",
    "    model.add(Dense(10, activation='softmax'))  # 10개의 클래스\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.01])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Keras Tuner Random Search 설정\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # 최적화 목표\n",
    "    max_trials=10,            # 시도할 하이퍼파라미터 조합 수\n",
    "    executions_per_trial=1,   # 각 조합 실행 횟수\n",
    "    directory='my_dir',       # 결과 저장 디렉토리\n",
    "    project_name='cnn_tuning' # 프로젝트 이름\n",
    ")\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_reshaped = X_train_combined.reshape(-1, 28, 28, 1)  # 4D 텐서로 변환\n",
    "X_valid_reshaped = X_valid.reshape(-1, 28, 28, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Random Search 실행\n",
    "tuner.search(X_train_reshaped, y_train_combined, \n",
    "             validation_data=(X_valid_reshaped, y_valid), \n",
    "             epochs=10, \n",
    "             batch_size=32)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# 최적 모델로 학습 및 평가\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_reshaped, y_train_combined, \n",
    "                         validation_data=(X_valid_reshaped, y_valid), \n",
    "                         epochs=10, \n",
    "                         batch_size=32)\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, cv=3, n_iter=5)\n",
    "random_search_result = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. 베이지안 최적화(Bayesian Optimization)**\n",
    "* 이전 탐색 결과를 바탕으로, 다음 탐색할 하이퍼파라미터 조합을 선택하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local Image](Contents/Bayesian_Search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "# Keras Tuner에서 사용할 모델 생성 함수\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters1', values=[32, 64]),  # 'filters1'에 대해 32 또는 64 선택\n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu', \n",
    "        input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters2', values=[64, 128]), # 'filters2'에 대해 64 또는 128 선택\n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout1', values=[0.2, 0.3]))) # 'dropout1'에 대해 0.2 또는 0.3 선택\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Choice('filters3', values=[128, 256]), \n",
    "        kernel_size=(3, 3), \n",
    "        activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(hp.Choice('dropout2', values=[0.3, 0.4])))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Choice('dense_units', values=[128, 256]), activation='relu'))\n",
    "    model.add(Dropout(hp.Choice('dropout3', values=[0.4, 0.5])))\n",
    "    model.add(Dense(10, activation='softmax'))  # 10개의 클래스\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.01])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Keras Tuner Bayesian Optimization 설정\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # 최적화 목표\n",
    "    max_trials=20,            # 시도할 하이퍼파라미터 조합 수\n",
    "    num_initial_points=5,     # 초기 랜덤 시도 횟수\n",
    "    directory='my_dir',       # 결과 저장 디렉토리\n",
    "    project_name='cnn_tuning_bayes' # 프로젝트 이름\n",
    ")\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_reshaped = X_train_combined.reshape(-1, 28, 28, 1)  # 4D 텐서로 변환\n",
    "X_valid_reshaped = X_valid.reshape(-1, 28, 28, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Bayesian Optimization 실행\n",
    "tuner.search(X_train_reshaped, y_train_combined, \n",
    "             validation_data=(X_valid_reshaped, y_valid), \n",
    "             epochs=10, \n",
    "             batch_size=32)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# 최적 모델로 학습 및 평가\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_reshaped, y_train_combined, \n",
    "                         validation_data=(X_valid_reshaped, y_valid), \n",
    "                         epochs=10, \n",
    "                         batch_size=32)\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "day16_1_jupyter",
   "language": "python",
   "name": "day16_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
